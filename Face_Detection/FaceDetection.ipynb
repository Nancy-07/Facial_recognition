{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Face Detection</h1>\n",
    "<h3 style=\"text-align:center\">By: <a href=\"https://github.com/Nancy-07\">Nancy Galicia</a>, <a href=\"https://github.com/AlvaroVasquezAI\">Álvaro García</a> and <a href=\"https://github.com/ConnorKenwayAC3\">Omar Sanchez</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 >Table of contents</h3>\n",
    "\n",
    "<div style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#introduction\">Introduction</a></li>\n",
    "        <li><a href=\"#libraries\">Libraries</a></li>\n",
    "        <li><a href=\"#data\">Data</a></li>\n",
    "        <li><a href=\"#model\">Model</a></li>\n",
    "        <li><a href=\"#trainingModel\">Training Model</a></li>\n",
    "        <li><a href=\"#evaluatingModel\">Evaluating Model</a></li>\n",
    "        <li><a href=\"#savingModel\">Saving Model</a></li>\n",
    "        <li><a href=\"#loadingModel\">Loading Model</a></li>\n",
    "        <li><a href=\"#testingModel\">Testing Model</a></li>\n",
    "        <li><a href=\"#conclusion\">Conclusion</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"introduction\" style=\"text-align:center\">Introduction</h2>\n",
    "This notebook demonstrates the creation of a face detection model using transfer learning. The process begins with labeling images to identify those with and without faces, followed by creating bounding boxes around the faces. A pre-trained model, MobileNetV2, serves as the base model. On top of MobileNetV2, a custom model is built specifically for face detection, which includes additional layers for classification and regression tasks. The model is trained with the labeled dataset, leveraging the power of transfer learning to improve accuracy and reduce training time. Finally, the performance of the model is evaluated using a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"libraries\" style=\"text-align:center\">Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data\" style=\"text-align:center\">Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Datasets</h3>\n",
    "\n",
    "<h4 style=\"text-align:center\">LFW | Jack</h4>\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/1_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/104_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Labeled Faces in the Wild</h3>\n",
    "\n",
    "The dataset \"Labeled Faces in the Wild\" (LFW) contains images of faces of famous people. This dataset is used for images that contain faces. The dataset contains 13,233 images of 5,749 people. \n",
    "\n",
    "We have reduce the size of the dataset to 11,917 images. This was done to perform the quality of the dataset, where we removed the images in which there were more than one face.\n",
    "\n",
    "The size of each image is 250x250 pixels.\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/1_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/64_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/71_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/102_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h3 style=\"text-align:center\">Jack</h3>\n",
    "\n",
    "The dataset \"Jack\" contains random images. The dataset contains 3,795 images. This dataset is used for images that do not contain faces, but some images contain faces, but they are not too clear or in a bad position.\n",
    "\n",
    "We resize the images to 250x250 pixels.\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/140_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/169_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/152_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/203_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Labeling data</h3>\n",
    "<h4 style=\"text-align:center\">Face | No face</h4>\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/64_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/203_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>\n",
    "\n",
    "We have labeled the data getting 2 coordinates, the top left and the bottom right of the bounding box of the face. The coordinates are normalized, so they are between 0 and 1. We have created a JSON file with the data of the images. \n",
    "\n",
    "The JSON file has the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"image\": \"file_name.jpg\",\n",
    "    \"bbox\": [\n",
    "        x1, \n",
    "        y1,\n",
    "        x2,\n",
    "        y2\n",
    "    ]\n",
    "    \"class\": 1|0\n",
    "}\n",
    "```\n",
    "where:\n",
    "- image: name of the image\n",
    "- bbox: coordinates of the bounding box (x1, y1, x2, y2), where (x1, y1) is the top left coordinate and (x2, y2) is the bottom right coordinate\n",
    "- class: 1 if the image contains a face, 0 if the image does not contain a face\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Split data</h3>\n",
    "\n",
    "We have split the data into training, validation and test sets. Where the training set contains 80% of the data, the validation set contains 10% of the data and the test set contains 10% of the data.\n",
    "\n",
    "<lo>\n",
    "    <li>Training set: 80% of the data</li>\n",
    "    <li>Validation set: 10% of the data</li>\n",
    "    <li>Test set: 10% of the data</li>\n",
    "</lo>\n",
    "\n",
    "Data distribution:\n",
    "\n",
    "- Training set: 9,588 images\n",
    "- Validation set: 1,200 images\n",
    "- Test set: 1,195 images\n",
    "\n",
    "<h4>Dataset structure</h4>\n",
    "\n",
    "The structure of the dataset is the following:\n",
    "\n",
    "```\n",
    "Data\n",
    "│\n",
    "└───Train\n",
    "│   │\n",
    "│   └───Images\n",
    "│   │   │   1_1.jpg\n",
    "│   │   │   2_1.jpg\n",
    "│   │   │   ...\n",
    "│   │\n",
    "│   └───Labels\n",
    "│       │   1_1.json\n",
    "│       │   2_1.json\n",
    "│       │   ...\n",
    "│\n",
    "└───Validation\n",
    "│   │\n",
    "│   └───Images\n",
    "│   │   │   1_1.jpg\n",
    "│   │   │   2_1.jpg\n",
    "│   │   │   ...\n",
    "│   │\n",
    "│   └───Labels\n",
    "│       │   1_1.json\n",
    "│       │   2_1.json\n",
    "│       │   ...\n",
    "│\n",
    "└───Test\n",
    "    │\n",
    "    └───Labels\n",
    "    │   │   1_1.jpg\n",
    "    │   │   2_1.jpg\n",
    "    │   │   ...\n",
    "    │\n",
    "    └───Labels\n",
    "        │   1_1.json\n",
    "        │   2_1.json\n",
    "        │   ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Download data</h3>\n",
    "\n",
    "To download the data, you can use the following link: <a href=\"#\">Data.zip</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Load data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center\">Are you using Google Colab?</h4>\n",
    "<p style=\"text-align:center\"> If you are using Google Colab, run the following code to load the data from Google Drive. </p>\n",
    "    \n",
    "```python  \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import zipfile\n",
    "\n",
    "zip_path = '/content/drive/My Drive/FaceDetection/Data.zip'\n",
    "!cp \"{zip_path}\" .\n",
    "!ls\n",
    "with zipfile.ZipFile('Data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('Dataset')\n",
    "!ls Dataset\n",
    "\n",
    "Data = 'Dataset/Data'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import zipfile\n",
    "\n",
    "zip_path = '/content/drive/My Drive/FaceDetection/Data.zip'\n",
    "!cp \"{zip_path}\" .\n",
    "!ls\n",
    "with zipfile.ZipFile('Data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('Dataset')\n",
    "!ls Dataset\n",
    "\n",
    "Data = 'Dataset/Data'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center\">Are you using a local environment?</h4>\n",
    "<p style=\"text-align:center\"> If you are using a local environment, just put the path where the data is located. </p>\n",
    "\n",
    "```python \n",
    "Data = 'path_folder_data'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = \"Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">How does the data look like?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDataset(path_image_with_face, path_image_without_face):\n",
    "    image_with_face = cv2.imread(path_image_with_face)\n",
    "    image_without_face = cv2.imread(path_image_without_face)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    path_label_with_face = path_image_with_face.replace(\"Images\", \"Labels\").replace(\".jpg\", \".json\")\n",
    "    path_label_without_face = path_image_without_face.replace(\"Images\", \"Labels\").replace(\".jpg\", \".json\")\n",
    "\n",
    "    with open(path_label_with_face) as json_file:\n",
    "        label_with_face = json.load(json_file)\n",
    "    with open(path_label_without_face) as json_file:\n",
    "        label_without_face = json.load(json_file)\n",
    "\n",
    "    name_with_face = label_with_face[\"image\"]\n",
    "    class_with_face = label_with_face[\"class\"]\n",
    "    coordinates_with_face = label_with_face[\"bbox\"]\n",
    "    x1_1 = coordinates_with_face[0] * image_with_face.shape[1]\n",
    "    y1_1 = coordinates_with_face[1] * image_with_face.shape[0]\n",
    "    x2_1 = coordinates_with_face[2] * image_with_face.shape[1]\n",
    "    y2_1 = coordinates_with_face[3] * image_with_face.shape[0]\n",
    "\n",
    "    name_without_face = label_without_face[\"image\"]\n",
    "    class_without_face = label_without_face[\"class\"]\n",
    "    coordinates_without_face = label_without_face[\"bbox\"]\n",
    "    x1_0 = coordinates_without_face[0] * image_without_face.shape[1]\n",
    "    y1_0 = coordinates_without_face[1] * image_without_face.shape[0]\n",
    "    x2_0 = coordinates_without_face[2] * image_without_face.shape[1]\n",
    "    y2_0 = coordinates_without_face[3] * image_without_face.shape[0]\n",
    "\n",
    "    cv2.rectangle(image_with_face, (int(x1_1), int(y1_1)), (int(x2_1), int(y2_1)), (0, 255, 0), 2)\n",
    "    ax[0].imshow(cv2.cvtColor(image_with_face, cv2.COLOR_BGR2RGB))\n",
    "    ax[0].set_title(f\"Image: {name_with_face}\\nClass: {class_with_face}\\nCoordinates: {coordinates_with_face}\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    cv2.rectangle(image_without_face, (int(x1_0), int(y1_0)), (int(x2_0), int(y2_0)), (0, 255, 0), 2)\n",
    "    ax[1].imshow(cv2.cvtColor(image_without_face, cv2.COLOR_BGR2RGB))\n",
    "    ax[1].set_title(f\"Image: {name_without_face}\\nClass: {class_without_face}\\nCoordinates: {coordinates_without_face}\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plotDataset(f\"{Data}/Test/Images/1_1.jpg\", f\"{Data}/Test/Images/8_0.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"model\" style=\"text-align:center\">Model</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\">MobileNetV2</h3>\n",
    "\n",
    "We will use the pre-trained model \"MobileNetV2\" to detect the faces in the images. The model is trained on the \"ImageNet\" dataset. The model has 155 layers and 3,504,872 parameters.\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"https://www.researchgate.net/publication/361260658/figure/fig1/AS:1179073011290112@1658124320675/The-architecture-of-MobileNetV2-DNN.png\" style=\"width: 500px; height: 500px\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Generating data</h3>\n",
    "\n",
    "Tensors for the training, validation and test sets. The tensors will contain the images, the bounding box coordinates and the class of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img\n",
    "\n",
    "train_images = tf.data.Dataset.list_files(f\"{Data}/Train/Images/*.jpg\", shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (224,224)))\n",
    "train_images = train_images.map(lambda x: x/255)\n",
    "\n",
    "validation_images = tf.data.Dataset.list_files(f\"{Data}/Validation/Images/*.jpg\", shuffle=False)\n",
    "validation_images = validation_images.map(load_image)\n",
    "validation_images = validation_images.map(lambda x: tf.image.resize(x, (224,224)))\n",
    "validation_images = validation_images.map(lambda x: x/255)\n",
    "\n",
    "test_images = tf.data.Dataset.list_files(f\"{Data}/Test/Images/*.jpg\", shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (224,224)))\n",
    "test_images = test_images.map(lambda x: x/255)\n",
    "\n",
    "train_images = train_images.map(lambda x: tf.ensure_shape(x, [224, 224, 3]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_images = validation_images.map(lambda x: tf.ensure_shape(x, [224, 224, 3]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_images = test_images.map(lambda x: tf.ensure_shape(x, [224, 224, 3]), num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(json_file_name):\n",
    "    with open(json_file_name.numpy(), 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return [data['class'], data['bbox']]\n",
    "\n",
    "def map_labels(x):\n",
    "    return tf.py_function(func=load_labels, inp=[x], Tout=[tf.uint8, tf.float32], name='map_labels')\n",
    "\n",
    "train_labels = tf.data.Dataset.list_files(f\"{Data}/Train/Labels/*.json\", shuffle=False)\n",
    "train_labels = train_labels.map(map_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_labels = train_labels.map(lambda class_, bbox: (tf.ensure_shape(class_, []), tf.ensure_shape(bbox, [4])), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "validation_labels = tf.data.Dataset.list_files(f\"{Data}/Validation/Labels/*.json\", shuffle=False)\n",
    "validation_labels = validation_labels.map(map_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_labels = validation_labels.map(lambda class_, bbox: (tf.ensure_shape(class_, []), tf.ensure_shape(bbox, [4])), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "test_labels = tf.data.Dataset.list_files(f\"{Data}/Test/Labels/*.json\", shuffle=False)\n",
    "test_labels = test_labels.map(map_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_labels = test_labels.map(lambda class_, bbox: (tf.ensure_shape(class_, []), tf.ensure_shape(bbox, [4])), num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data (Images and labels together | Training, Validation and Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(9600)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "validation = tf.data.Dataset.zip((validation_images, validation_labels))\n",
    "validation = validation.shuffle(1200)\n",
    "validation = validation.batch(8)\n",
    "validation = validation.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1200)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = train.as_numpy_iterator()\n",
    "res = data_sample.next()\n",
    "\n",
    "for idx in range(4): \n",
    "    Image = res[0][idx]\n",
    "    class_ = res[1][0][idx]\n",
    "    coords = res[1][1][idx]\n",
    "\n",
    "    x1 = int(coords[0] * 224)\n",
    "    y1 = int(coords[1] * 224)\n",
    "    x2 = int(coords[2] * 224)\n",
    "    y2 = int(coords[3] * 224)\n",
    "\n",
    "    Image = Image * 255\n",
    "    Image = Image.astype(np.uint8)\n",
    "\n",
    "    cv2.rectangle(Image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    plt.title(class_)\n",
    "    plt.imshow(Image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Building model</h3>\n",
    "\n",
    "To create a custom face detection model, the pre-trained \"MobileNetV2\" is utilized as the base model. This approach leverages the features learned from the ImageNet dataset while adding custom layers to suit the specific task of face detection. The model consists of one input layer and two output layers, tailored for classification and regression.\n",
    "\n",
    "Input layers:\n",
    "1. Input: The input layer accepts images of the shape (224, 224, 3), which is the required input size for MobileNetV2. This layer handles the input images that are fed into the model.\n",
    "\n",
    "Base model:\n",
    "1. MobileNetV2: MobileNetV2 serves as the base model, leveraging pre-trained weights from ImageNet. By setting include_top=False, the fully connected layers at the top of MobileNetV2 are excluded, allowing the addition of custom layers that cater to the specific detection task.\n",
    "\n",
    "Output layers:\n",
    "1. Classification Output:\n",
    "- Neurons: 1 neuron\n",
    "- Function: This neuron predicts whether the input image contains a face or not.\n",
    "- Activation: Sigmoid activation function, which outputs a probability between 0 and 1 indicating the presence of a face.\n",
    "\n",
    "2. Regression: This output will have 4 neurons, which will predict the bounding box coordinates. The activation function will be \"sigmoid\".\n",
    "- Neurons: 4 neurons\n",
    "- Function: These neurons predict the coordinates of the bounding box surrounding the face.\n",
    "- Activation: Sigmoid activation function, which normalizes the output to a range between 0 and 1, representing the bounding box coordinates relative to the input image size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))(input_layer)\n",
    "\n",
    "    f1 = GlobalMaxPooling2D()(base_model)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid', name='classification')(class1)\n",
    "\n",
    "    f2 = GlobalMaxPooling2D()(base_model)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid', name='bounding_box')(regress1)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFaceDetection = build_model()\n",
    "modelFaceDetection.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Face Detection</h3>\n",
    "\n",
    "<p>The face detection model uses MobileNetV2 as the base model, adding custom layers for classification and regression. This allows the model to detect faces and predict bounding box coordinates.</p>\n",
    "\n",
    "#### Loss Functions\n",
    "- Regression Loss: Measures the error between the true and predicted bounding box coordinates.\n",
    "- Classification Loss: Uses binary cross-entropy to measure the error in face detection.\n",
    "#### Optimizer\n",
    "- An Adam optimizer with an exponential decay learning rate schedule.\n",
    "#### Face Detection Class\n",
    "- Defines a custom model for face detection with specific training and evaluation steps.\n",
    "#### Model Initialization\n",
    "- Initialize and compile the <code>FaceDetection</code> model with the custom optimizer and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions & Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"face_detection\")\n",
    "def regression_loss(y_true, yhat):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "    h_true = y_true[:,3] - y_true[:,1]\n",
    "    w_true = y_true[:,2] - y_true[:,0]\n",
    "    h_pred = yhat[:,3] - yhat[:,1]\n",
    "    w_pred = yhat[:,2] - yhat[:,0]\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "    return delta_coord + delta_size\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"face_detection\")\n",
    "def classification_loss():\n",
    "    return tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"face_detection\")\n",
    "def optimizerpro():\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    return opt\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"face_detection\")\n",
    "class FaceDetection(Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def compile(self, optimizer, classloss, regressloss):\n",
    "        super().compile()\n",
    "        self.opt = optimizer\n",
    "        self.closs = classloss\n",
    "        self.rloss = regressloss\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        X, y = batch\n",
    "        \n",
    "        y_class = tf.reshape(y[0], (-1, 1))\n",
    "        y_bbox = tf.cast(y[1], tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.model(X, training=True)\n",
    "\n",
    "            batch_classloss = self.closs(y_class, classes)\n",
    "            batch_regressloss = self.rloss(y_bbox, coords)\n",
    "            total_loss = 1.5 * batch_regressloss + 0.5 * batch_classloss\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_regressloss}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        X, y = batch\n",
    "\n",
    "        y_class = tf.reshape(y[0], (-1, 1))\n",
    "        y_bbox = tf.cast(y[1], tf.float32)\n",
    "\n",
    "        classes, coords = self.model(X, training=False)\n",
    "\n",
    "        batch_classloss = self.closs(y_class, classes)\n",
    "        batch_regressloss = self.rloss(y_bbox, coords)\n",
    "        total_loss = 1.5 * batch_regressloss + 0.5 * batch_classloss\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_regressloss}\n",
    "\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"model\": self.model.get_config(),\n",
    "            \"optimizer\": tf.keras.utils.serialize_keras_object(self.opt),\n",
    "            \"classloss\": tf.keras.utils.serialize_keras_object(self.closs),\n",
    "            \"regressloss\": tf.keras.utils.serialize_keras_object(self.rloss),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, custom_objects=None):\n",
    "        model = Model.from_config(config[\"model\"], custom_objects=custom_objects)\n",
    "        optimizer = tf.keras.utils.deserialize_keras_object(config[\"optimizer\"], custom_objects=custom_objects)\n",
    "        classloss = tf.keras.utils.deserialize_keras_object(config[\"classloss\"], custom_objects=custom_objects)\n",
    "        regressloss = tf.keras.utils.deserialize_keras_object(config[\"regressloss\"], custom_objects=custom_objects)\n",
    "        instance = cls(model)\n",
    "        instance.compile(optimizer=optimizer, classloss=classloss, regressloss=regressloss)\n",
    "        return instance\n",
    "\n",
    "    def get_compile_config(self):\n",
    "        return {\n",
    "            \"optimizer\": self.opt,\n",
    "            \"classloss\": self.closs,\n",
    "            \"regressloss\": self.rloss,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def compile_from_config(cls, config):\n",
    "        optimizer = tf.keras.utils.deserialize_keras_object(config[\"optimizer\"])\n",
    "        classloss = tf.keras.utils.deserialize_keras_object(config[\"classloss\"])\n",
    "        regressloss = tf.keras.utils.deserialize_keras_object(config[\"regressloss\"])\n",
    "        return optimizer, classloss, regressloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Detection Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Face_Detection = FaceDetection(modelFaceDetection)\n",
    "Face_Detection.compile(optimizer=optimizerpro(), classloss=classification_loss(), regressloss=regression_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"trainingModel\" style=\"text-align:center\">Training Model</h2>\n",
    "\n",
    "The <code>Face_Detection.fit</code> method trains the model using the training data (<code>train</code>) and validation data (<code>validation</code>) for n epochs. The <code>tensorboard_callback</code> logs the metrics during training for later visualization in TensorBoard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Face_Detection.fit(train, validation_data=validation, epochs=10, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"evaluatingModel\" style=\"text-align:center\">Evaluating Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(history.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(history.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(history.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(history.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(history.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Face_Detection.evaluate(test)\n",
    "print(f\"Results: {results}\")\n",
    "\n",
    "if int(len(results)) == 3:\n",
    "    print(f\"Loss: {results[0]}, Classification Accuracy: {results[1]}, Bounding Box MSE: {results[2]}\")\n",
    "else:\n",
    "    print(\"Unexpected results format:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"savingModel\" style=\"text-align:center\">Saving Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Face_Detection.save('FaceDetectionModel.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"loadingModel\" style=\"text-align:center\">Loading Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded_FaceDetection = tf.keras.models.load_model('FaceDetectionModel.keras')\n",
    "\n",
    "model_loaded_FaceDetection.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"testingModel\" style=\"text-align:center\">Testing Model</h2>\n",
    "<h3 style=\"text-align:center\">Using the Test Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "X, y = test.as_numpy_iterator().next()\n",
    "predictions = model_loaded_FaceDetection.predict(X)\n",
    "\n",
    "for idx in range(4):\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "\n",
    "    Image = X[idx]\n",
    "    class_ = y[0][idx]\n",
    "    coords = y[1][idx]\n",
    "\n",
    "    x1 = int(coords[0] * 224)\n",
    "    y1 = int(coords[1] * 224)\n",
    "    x2 = int(coords[2] * 224)\n",
    "    y2 = int(coords[3] * 224)\n",
    "\n",
    "    print(f\"Actual Class: {class_}, Actual Coordinates: {coords}\")\n",
    "\n",
    "    Image = Image * 255\n",
    "    Image = Image.astype(np.uint8)\n",
    "    ImagePred = Image.copy()\n",
    "\n",
    "    cv2.rectangle(Image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    ax[0].title.set_text(class_)\n",
    "    ax[0].imshow(Image)\n",
    "\n",
    "    class_pred = predictions[0][idx]\n",
    "    coords_pred = predictions[1][idx]\n",
    "\n",
    "    x1_pred = int(coords_pred[0] * 224)\n",
    "    y1_pred = int(coords_pred[1] * 224)\n",
    "    x2_pred = int(coords_pred[2] * 224)\n",
    "    y2_pred = int(coords_pred[3] * 224)\n",
    "\n",
    "    print(f\"Predicted Class: {class_pred}, Predicted Coordinates: {coords_pred}\")\n",
    "\n",
    "    cv2.rectangle(ImagePred, (x1_pred, y1_pred), (x2_pred, y2_pred), (0, 255, 0), 2)\n",
    "    ax[1].title.set_text(class_pred)\n",
    "    ax[1].imshow(ImagePred)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Using new images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_image(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "\n",
    "    img_normalized = img_resized / 255.0\n",
    "    \n",
    "    img_expanded = np.expand_dims(img_normalized, axis=0)\n",
    "    \n",
    "    return img_expanded, img_resized\n",
    "\n",
    "image_paths = [\"ImagesTest/face1.jpg\",\"ImagesTest/Omar.png\", \"ImagesTest/Sarita.png\"]\n",
    "external_images = [load_and_preprocess_image(path) for path in image_paths]\n",
    "\n",
    "for img_expanded, img_original in external_images:\n",
    "    predictions = model_loaded_FaceDetection.predict(img_expanded)\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    \n",
    "    Image = img_original.copy()\n",
    "    ImagePred = img_original.copy()\n",
    "\n",
    "    class_pred = predictions[0][0]\n",
    "    coords_pred = predictions[1][0]\n",
    "\n",
    "    x1_pred = int(coords_pred[0] * 224)\n",
    "    y1_pred = int(coords_pred[1] * 224)\n",
    "    x2_pred = int(coords_pred[2] * 224)\n",
    "    y2_pred = int(coords_pred[3] * 224)\n",
    "\n",
    "    print(f\"Predicted Class: {class_pred}, Predicted Coordinates: {coords_pred}\")\n",
    "\n",
    "    cv2.rectangle(ImagePred, (x1_pred, y1_pred), (x2_pred, y2_pred), (0, 255, 0), 2)\n",
    "    ax[0].title.set_text(\"Original Image\")\n",
    "    ax[0].imshow(Image)\n",
    "    ax[1].title.set_text(f\"Predicted Class: {class_pred[0]}\")\n",
    "    ax[1].imshow(ImagePred)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Using the webcam in real time</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image_camera(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "\n",
    "    img_normalized = img_resized / 255.0\n",
    "    \n",
    "    img_expanded = np.expand_dims(img_normalized, axis=0)\n",
    "    \n",
    "    return img_expanded, img_resized\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_expanded, img_original = load_and_preprocess_image_camera(frame)\n",
    "    predictions = model_loaded_FaceDetection.predict(img_expanded)\n",
    "    \n",
    "    class_pred = predictions[0][0]\n",
    "    coords_pred = predictions[1][0]\n",
    "\n",
    "    x1_pred = int(coords_pred[0] * 224)\n",
    "    y1_pred = int(coords_pred[1] * 224)\n",
    "    x2_pred = int(coords_pred[2] * 224)\n",
    "    y2_pred = int(coords_pred[3] * 224)\n",
    "\n",
    "    cv2.rectangle(img_original, (x1_pred, y1_pred), (x2_pred, y2_pred), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Face Detection\", cv2.cvtColor(img_original, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"conclusion\" style=\"text-align:center\">Conclusion</h2>\n",
    "\n",
    "Face detection is a crucial task in computer vision, with applications in security, surveillance, and image processing. This notebook demonstrates the creation of a face detection model using transfer learning, leveraging the pre-trained MobileNetV2 model. By adding custom layers for classification and regression, the model can accurately detect faces and predict bounding box coordinates. The model is trained on a labeled dataset, achieving high accuracy and performance. The face detection model can be used for various applications, such as facial recognition, emotion detection, and face trackin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArtificialIntelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
