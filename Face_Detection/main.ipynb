{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Face Detection</h1>\n",
    "<h3 style=\"text-align:center\">By: <a href=\"https://www.kaggle.com/rafanthx13\">Nancy Galicia</a>, <a href=\"https://www.kaggle.com/rafaelgreca\">Álvaro García</a> and <a href=\"https://www.kaggle.com/rafaelmm\">Omar Sanchez</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 >Table of contents</h3>\n",
    "\n",
    "<div style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#introduction\">Introduction</a></li>\n",
    "        <li><a href=\"#libraries\">Libraries</a></li>\n",
    "        <li><a href=\"#data\">Data</a></li>\n",
    "        <li><a href=\"#model\">Model</a></li>\n",
    "        <li><a href=\"#training\">Training</a></li>\n",
    "        <li><a href=\"#evaluation\">Evaluation</a></li>\n",
    "        <li><a href=\"#conclusion\">Conclusion</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"introduction\" style=\"text-align:center\">Introduction</h2>\n",
    "\n",
    "In this notebook we will create a face detection model. We will use the dataset \"Labelled Faces in the Wild\" (LFW) which contains images of faces of famous people, this is for images that contain faces. For images that do not contain faces, we will use the dataset \"Jack\", which contains random images. After getting the data, we will label it getting the bounding box of the faces. Then, we will use a pre-trained model to detect the faces in the images. We will use \"MobileNetV2\" as the pre-trained model. The model will be trained using the dataset we created. The technique we will use is transfer learning. Finally, we will evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"libraries\" style=\"text-align:center\">Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "# For create the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data\" style=\"text-align:center\">Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Datasets</h3>\n",
    "\n",
    "<h4 style=\"text-align:center\">LFW | Jack</h4>\n",
    "\n",
    "The datasets we will use are the following:\n",
    "\n",
    "- LFW: This dataset contains images of faces of famous people. It contains 13,233 images of 5,749 people. This dataset is used for images that contain faces.\n",
    "\n",
    "- Jack: This dataset contains random images. It contains 3,795 images. This dataset is used for images that do not contain faces.\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/1_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/104_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Labeled Faces in the Wild</h3>\n",
    "\n",
    "The dataset \"Labeled Faces in the Wild\" (LFW) contains images of faces of famous people. This dataset is used for images that contain faces. The dataset contains 13,233 images of 5,749 people. \n",
    "\n",
    "We have reduce the size of the dataset to 11,917 images. This was done to perform the quality of the dataset, where we removed the images in which there were more than one face.\n",
    "\n",
    "The size of each image is 250x250 pixels.\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/1_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/64_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/71_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/102_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h3 style=\"text-align:center\">Jack</h3>\n",
    "\n",
    "The dataset \"Jack\" contains random images. The dataset contains 3,795 images. This dataset is used for images that do not contain faces, but some images contain faces, but they are not too clear or in a bad position.\n",
    "\n",
    "We resize the images to 250x250 pixels.\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/140_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/169_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/152_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/203_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Labeling data</h3>\n",
    "<h4 style=\"text-align:center\">Face | No face</h4>\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"Data/Test/Images/64_1.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "    <img src=\"Data/Test/Images/203_0.jpg\" style=\"width: 250px; height: 250px\"/>\n",
    "</div>\n",
    "\n",
    "We have labeled the data getting 2 coordinates, the top left and the bottom right of the bounding box of the face. The coordinates are normalized, so they are between 0 and 1. We have created a JSON file with the data of the images. \n",
    "\n",
    "The JSON file has the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"image\": \"file_name.jpg\",\n",
    "    \"bbox\": [\n",
    "        x1, \n",
    "        y1,\n",
    "        x2,\n",
    "        y2\n",
    "    ]\n",
    "    \"class\": 1|0\n",
    "}\n",
    "```\n",
    "where:\n",
    "- image: name of the image\n",
    "- bbox: coordinates of the bounding box (x1, y1, x2, y2), where (x1, y1) is the top left coordinate and (x2, y2) is the bottom right coordinate\n",
    "- class: 1 if the image contains a face, 0 if the image does not contain a face\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center\">Code</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the coordinates of the face from the dataframe previously created and returns them, this csv file contains the coordinates of the face in the image\n",
    "def get_coordinates(dataframe: pd.DataFrame, image_name: str):\n",
    "    index = int(image_name.split('.')[0])\n",
    "\n",
    "    face = dataframe.iloc[index]\n",
    "    x, y, w, h = face['X'], face['Y'], face['Width'], face['Height']\n",
    "\n",
    "    x1 = x\n",
    "    y1 = y\n",
    "    x2 = x + w\n",
    "    y2 = y + h\n",
    "\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def plot_image(image, coordinates):\n",
    "    x1, y1, x2, y2 = coordinates\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def createLabel(filename: str, bbox: list, class_id: int, json_file_name: str, img_size: list, json_folder: str ):\n",
    "    # Convert the bbox to float\n",
    "    bbox = [float(coord) for coord in bbox]\n",
    "\n",
    "    # Normalize the bbox coordinates (values between 0 and 1)\n",
    "    width, height = img_size   \n",
    "\n",
    "    bbox[0] = bbox[0] / width\n",
    "    bbox[1] = bbox[1] / height\n",
    "    bbox[2] = bbox[2] / width\n",
    "    bbox[3] = bbox[3] / height\n",
    "\n",
    "    data = {\n",
    "        \"image\": filename,\n",
    "        \"bbox\": bbox,\n",
    "        \"class\": class_id\n",
    "    }\n",
    "    if not os.path.exists(json_folder):\n",
    "        os.makedirs(json_folder)\n",
    "\n",
    "    with open(f\"{json_folder}/{json_file_name}\", 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "def readLabelPlotImage(json_file_name: str, dataset: str):\n",
    "    with open(json_file_name, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    image = cv2.imread(f\"{dataset}/{data['image']}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bbox = data['bbox']\n",
    "\n",
    "    x1 = int(bbox[0] * 250)\n",
    "    y1 = int(bbox[1] * 250)\n",
    "    x2 = int(bbox[2] * 250)\n",
    "    y2 = int(bbox[3] * 250)\n",
    "\n",
    "    print(x1, y1, x2, y2)\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readLabelPlotImage(\"Data/Test/Labels/1_1.json\", \"Data/Test/Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Split data</h3>\n",
    "\n",
    "We have split the data into training, validation and test sets. Where the training set contains 80% of the data, the validation set contains 10% of the data and the test set contains 10% of the data.\n",
    "\n",
    "<lo>\n",
    "    <li>Training set: 80% of the data</li>\n",
    "    <li>Validation set: 10% of the data</li>\n",
    "    <li>Test set: 10% of the data</li>\n",
    "</lo>\n",
    "\n",
    "In the split we have taken into account that the data is balanced, that is, the training, validation and test sets contain the same number of images with faces and without faces. So we have around 11000 images for faces and 3000 images for no faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"model\" style=\"text-align:center\">Model</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\">MobileNetV2</h3>\n",
    "\n",
    "We will use the pre-trained model \"MobileNetV2\" to detect the faces in the images. The model is trained on the \"ImageNet\" dataset. The model has 155 layers and 3,504,872 parameters.\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"https://www.researchgate.net/publication/361260658/figure/fig1/AS:1179073011290112@1658124320675/The-architecture-of-MobileNetV2-DNN.png\" style=\"width: 500px; height: 500px\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"justify-content:center; display:flex\">\n",
    "    <img src=\"https://www.researchgate.net/publication/342856036/figure/fig3/AS:911929400885251@1594432320422/The-architecture-of-the-MobileNetv2-network.ppm\" style=\"width: 500px; height: 250px\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar MobileNetV2 preentrenado en ImageNet, excluyendo las capas superiores\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Congelar las capas base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Añadir nuevas capas para la clasificación y la regresión\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Clasificación binaria (rostro/no rostro)\n",
    "classification_output = Dense(1, activation='sigmoid', name='classification')(x)\n",
    "\n",
    "# Regresión para el bounding box\n",
    "regression_output = Dense(4, activation='sigmoid', name='bounding_box')(x)\n",
    "\n",
    "# Crear el modelo final con dos salidas\n",
    "model = Model(inputs=base_model.input, outputs=[classification_output, regression_output])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'classification': 'binary_crossentropy', 'bounding_box': 'mean_squared_error'},\n",
    "              metrics={'classification': 'accuracy', 'bounding_box': 'mse'})\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_folder, label_folder, batch_size=32, dim=(224, 224), shuffle=True):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.image_ids = [f.split('.')[0] for f in os.listdir(image_folder)]\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.image_ids[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y_class, y_bbox = self.__data_generation(batch_ids)\n",
    "        return X, {'classification': y_class, 'bounding_box': y_bbox}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.image_ids)\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        X = np.empty((self.batch_size, *self.dim, 3))\n",
    "        y_class = np.empty((self.batch_size, 1))\n",
    "        y_bbox = np.empty((self.batch_size, 4))\n",
    "\n",
    "        for i, id in enumerate(batch_ids):\n",
    "            img_path = os.path.join(self.image_folder, id + '.jpg')\n",
    "            label_path = os.path.join(self.label_folder, id + '.json')\n",
    "            \n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "                \n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, self.dim)\n",
    "            img = img / 255.0  # Normalización\n",
    "\n",
    "            X[i,] = img\n",
    "            y_class[i,] = label['class']\n",
    "            y_bbox[i,] = label['bbox']\n",
    "\n",
    "\n",
    "        return X, y_class, y_bbox\n",
    "\n",
    "# Crear los generadores de datos\n",
    "train_generator = DataGenerator('Data/Train/Images', 'Data/Train/Labels')\n",
    "val_generator = DataGenerator('Data/Validation/Images', 'Data/Validation/Labels')\n",
    "test_generator = DataGenerator('Data/Test/Images', 'Data/Test/Labels', shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"training\" style=\"text-align:center\">Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=200,  # Ajusta el número de épocas según tus necesidades\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(val_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"evaluation\" style=\"text-align:center\">Evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "results = model.evaluate(test_generator)\n",
    "if len(results) == 3:\n",
    "    print(f\"Loss: {results[0]}, Classification Accuracy: {results[1]}, Bounding Box MSE: {results[2]}\")\n",
    "else:\n",
    "    print(\"Unexpected results format:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "model.save('face_detection_v2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">Using model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(image_path, model):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_resized = img_resized / 255.0  # Normalización\n",
    "    img_resized = np.expand_dims(img_resized, axis=0)\n",
    "\n",
    "    pred_class, pred_bbox = model.predict(img_resized)\n",
    "    pred_class = pred_class[0][0]\n",
    "    pred_bbox = pred_bbox[0]\n",
    "\n",
    "    if pred_class > 0.5:\n",
    "        h, w, _ = img.shape\n",
    "        x1 = int(pred_bbox[0] * w)\n",
    "        y1 = int(pred_bbox[1] * h)\n",
    "        x2 = int(pred_bbox[2] * w)\n",
    "        y2 = int(pred_bbox[3] * h)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No face detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('face_detection_v1.keras') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"conclusion\" style=\"text-align:center\">Conclusion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImage(json_file_name: str, dataset: str):\n",
    "    with open(json_file_name, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    w, h = 400, 400\n",
    "    image = cv2.imread(f\"{dataset}/{data['image']}\")\n",
    "    image = cv2.resize(image, (w, h))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bbox = data['bbox']\n",
    "\n",
    "    x1 = int(bbox[0] * w)\n",
    "    y1 = int(bbox[1] * h)\n",
    "    x2 = int(bbox[2] * w)\n",
    "    y2 = int(bbox[3] * h)\n",
    "\n",
    "    print(x1, y1, x2, y2)\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "plotImage('Data/Test/Labels/1_1.json', 'Data/Test/Images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArtificialIntelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
